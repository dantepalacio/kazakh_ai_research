{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d1e1799-998a-41a8-9fad-4128ea5f36f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
      "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.25.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.10.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.9.11)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (15.0.7)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.5)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (5.28.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[sentencepiece]) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[sentencepiece]) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.25.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.25.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.0.1+cu118)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (15.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets sacrebleu torch transformers \n",
    "! pip install sentencepiece transformers[sentencepiece]\n",
    "! pip install accelerate -U\n",
    "!pip install scikit-learn datasets peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6e1d8a-f7a1-4ecf-81b9-76a8e69d7c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from datasets import Dataset\n",
    "# from datasets import load_metric\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast #mbart\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5647509-363d-488e-ad46-6ad317fa8298",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "BLEU = \"bleu\"\n",
    "KAZAKH = \"kk\"\n",
    "# KAZAKH_TEXT = \"kazakh_text\"\n",
    "EPOCH = \"epoch\"\n",
    "INPUT_IDS = \"input_ids\"\n",
    "GEN_LEN = \"gen_len\"\n",
    "MAX_INPUT_LENGTH = 128\n",
    "MAX_TARGET_LENGTH = 128\n",
    "MODEL_CHECKPOINT = 'facebook/mbart-large-cc25'\n",
    "MODEL_NAME = MODEL_CHECKPOINT.split(\"/\")[-1]\n",
    "LABELS = \"labels\"\n",
    "PREFIX = \"\"\n",
    "RUSSIAN = \"ru\"\n",
    "# RUSSIAN_TEXT = \"russian_text\"\n",
    "SCORE = \"score\"\n",
    "SOURCE_LANG = \"kk\"\n",
    "TARGET_LANG = \"eu\"\n",
    "TRANSLATION = \"translation\"\n",
    "# UNNAMED_COL = \"Unnamed: 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68fd36b5-1337-4745-ad6a-78b14c77e4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "login('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')\n",
    "\n",
    "dataset = load_dataset('issai/kazparc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "743f21da-75be-427b-a511-f0831d08c598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8000 entries, 0 to 7999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      8000 non-null   object\n",
      " 1   kk      8000 non-null   object\n",
      " 2   en      8000 non-null   object\n",
      " 3   ru      8000 non-null   object\n",
      " 4   tr      8000 non-null   object\n",
      " 5   domain  8000 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 375.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.DataFrame(dataset['train'])\n",
    "\n",
    "df_train = df_train.head(8000)\n",
    "\n",
    "df_train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88192c32-3014-443b-b857-10d3f2ac5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_empty_rows(example):\n",
    "#     return example['kk'].strip() != '' and example['ru'].strip() != ''\n",
    "\n",
    "# filtered_dataset = dataset.filter(filter_empty_rows)\n",
    "\n",
    "# x = filtered_dataset['train']['kk']\n",
    "# y = filtered_dataset['train']['ru']\n",
    "\n",
    "# print(f\"Количество элементов в 'kk': {len(x)}\")\n",
    "# print(f\"Количество элементов в 'ru': {len(y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a0d4832-5f0e-4435-b5c7-93cc04964089",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_train['kk']\n",
    "y = df_train['ru']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81aae6be-8304-42d9-9dc2-3ae291f31fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.10, shuffle=True, random_state=100)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.05, shuffle=True, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bde7467-528a-44cd-a2f2-33b8a03eb8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL X-TRAIN SHAPE:  (6840,)\n",
      "INITIAL Y-TRAIN SHAPE:  (6840,)\n",
      "X-TEST SHAPE:  (800,)\n",
      "Y-TEST SHAPE:  (800,)\n"
     ]
    }
   ],
   "source": [
    "print(\"INITIAL X-TRAIN SHAPE: \", x_train.shape)\n",
    "print(\"INITIAL Y-TRAIN SHAPE: \", y_train.shape)\n",
    "print(\"X-TEST SHAPE: \", x_test.shape)\n",
    "print(\"Y-TEST SHAPE: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6693231c-b34e-4ab7-b713-b60cb4f21651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL X-TRAIN SHAPE:  (6840,)\n",
      "FINAL Y-TRAIN SHAPE:  (6840,)\n",
      "X-VAL SHAPE:  (360,)\n",
      "Y-VAL SHAPE:  (360,)\n"
     ]
    }
   ],
   "source": [
    "print(\"FINAL X-TRAIN SHAPE: \", x_train.shape)\n",
    "print(\"FINAL Y-TRAIN SHAPE: \", y_train.shape)\n",
    "print(\"X-VAL SHAPE: \", x_val.shape)\n",
    "print(\"Y-VAL SHAPE: \", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e95704e1-ef00-41c5-9e4a-5b888ec2285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds: list, labels: list) -> tuple:\n",
    "    \"\"\"Performs post processing on the prediction text and labels\"\"\"\n",
    "\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def prep_data_for_model_fine_tuning(source_lang: list, target_lang: list) -> list:\n",
    "    \"\"\"Takes the input data lists and converts into translation list of dicts\"\"\"\n",
    "\n",
    "    data_dict = dict()\n",
    "    data_dict[TRANSLATION] = []\n",
    "\n",
    "    for sr_text, tr_text in zip(source_lang, target_lang):\n",
    "        temp_dict = dict()\n",
    "        temp_dict[RUSSIAN] = sr_text\n",
    "        temp_dict[KAZAKH] = tr_text\n",
    "\n",
    "        data_dict[TRANSLATION].append(temp_dict)\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def generate_model_ready_dataset(dataset: list, source: str, target: str,\n",
    "                                 model_checkpoint: str,\n",
    "                                 tokenizer: AutoTokenizer):\n",
    "    \"\"\"Makes the data training ready for the model\"\"\"\n",
    "\n",
    "    preped_data = []\n",
    "\n",
    "    for row in dataset:\n",
    "        inputs = PREFIX + row[source]\n",
    "        targets = row[target]\n",
    "\n",
    "        model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH,\n",
    "                                 truncation=True, padding=True)\n",
    "\n",
    "        model_inputs[TRANSLATION] = row\n",
    "\n",
    "        # setup the tokenizer for targets\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, max_length=MAX_INPUT_LENGTH,\n",
    "                                 truncation=True, padding=True)\n",
    "            model_inputs[LABELS] = labels[INPUT_IDS]\n",
    "\n",
    "        preped_data.append(model_inputs)\n",
    "\n",
    "    return preped_data\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds: tuple) -> dict:\n",
    "    \"\"\"computes bleu score and other performance metrics \"\"\"\n",
    "\n",
    "    # metric = datasets.load_metric(\"sacrebleu\")\n",
    "    metric = evaluate.load(\"sacrebleu\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {BLEU: result[SCORE]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "\n",
    "    result[GEN_LEN] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fe80557-40d6-4475-b5f6-2d722466ff4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBartTokenizer'. \n",
      "The class this function is called from is 'MBart50TokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_CHECKPOINT, src_lang=\"kk_KZ\", tgt_lang=\"ru_RU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "761983ce-42e5-4de6-8309-b9ee5d355e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = prep_data_for_model_fine_tuning(x_train.values, y_train.values)\n",
    "\n",
    "validation_data = prep_data_for_model_fine_tuning(x_val.values, y_val.values)\n",
    "\n",
    "test_data = prep_data_for_model_fine_tuning(x_test.values, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58af2705-4af1-4aac-825c-ba43e4beba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = generate_model_ready_dataset(dataset=training_data[TRANSLATION],\n",
    "                                          tokenizer=tokenizer,\n",
    "                                          source=KAZAKH,\n",
    "                                          target=RUSSIAN,\n",
    "                                          model_checkpoint=MODEL_CHECKPOINT)\n",
    "\n",
    "validation_data = generate_model_ready_dataset(dataset=validation_data[TRANSLATION],\n",
    "                                               tokenizer=tokenizer,\n",
    "                                               source=KAZAKH,\n",
    "                                               target=RUSSIAN,\n",
    "                                               model_checkpoint=MODEL_CHECKPOINT)\n",
    "\n",
    "test_data = generate_model_ready_dataset(dataset=test_data[TRANSLATION],\n",
    "                                               tokenizer=tokenizer,\n",
    "                                               source=KAZAKH,\n",
    "                                               target=RUSSIAN,\n",
    "                                               model_checkpoint=MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b609e6b3-0840-4bbb-bc7b-2abdf28d0f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6840 entries, 0 to 6839\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   attention_mask  6840 non-null   object\n",
      " 1   input_ids       6840 non-null   object\n",
      " 2   labels          6840 non-null   object\n",
      " 3   translation     6840 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 213.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame.from_records(train_data)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a1912e1-2b0c-4b4d-ab61-499506c2634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 360 entries, 0 to 359\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   attention_mask  360 non-null    object\n",
      " 1   input_ids       360 non-null    object\n",
      " 2   labels          360 non-null    object\n",
      " 3   translation     360 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 11.4+ KB\n"
     ]
    }
   ],
   "source": [
    "validation_df = pd.DataFrame.from_records(validation_data)\n",
    "validation_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28047cb0-fe0b-47fc-806b-3a5376703f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 800 entries, 0 to 799\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   attention_mask  800 non-null    object\n",
      " 1   input_ids       800 non-null    object\n",
      " 2   labels          800 non-null    object\n",
      " 3   translation     800 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 25.1+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_df = pd.DataFrame.from_records(test_data)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21125af3-3f58-4caf-9327-072cc36310f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'labels', 'translation'],\n",
       "    num_rows: 6840\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb57cee0-8774-4e58-954d-b0574363fefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'labels', 'translation'],\n",
       "    num_rows: 360\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6eedc296-f0d6-4991-93aa-46b8a3628e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'labels', 'translation'],\n",
       "    num_rows: 800\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "548e5349-92b8-4d99-b6ef-61c9a7361f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4244b89-cf03-49c2-bbb0-baff786c4673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "611736576"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f15f628d-a3d3-4571-936e-cf13a939f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=6,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c6b9c33-81cd-4b27-8f32-8fc8c0d4a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = Seq2SeqTrainingArguments(\n",
    "    f\"{MODEL_NAME}-finetuned-{SOURCE_LANG}-to-{TARGET_LANG}\",\n",
    "    # evaluation_strategy=\"steps\", \n",
    "    # eval_steps=2000,\n",
    "    evaluation_strategy=EPOCH,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    weight_decay=0.02,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10222450-4202-4dd3-a6eb-49b277b68f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffa7f37f-49d7-475b-ad8f-75ecbd848d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    model_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d7c86bc-6a61-44cf-af76-57ce257946b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4275' max='4275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4275/4275 1:27:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.934500</td>\n",
       "      <td>2.040852</td>\n",
       "      <td>1.722000</td>\n",
       "      <td>244.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.271900</td>\n",
       "      <td>1.893594</td>\n",
       "      <td>1.555800</td>\n",
       "      <td>75.566700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.141000</td>\n",
       "      <td>1.844154</td>\n",
       "      <td>1.600700</td>\n",
       "      <td>102.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.073200</td>\n",
       "      <td>1.802077</td>\n",
       "      <td>3.321600</td>\n",
       "      <td>57.988900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.015100</td>\n",
       "      <td>1.796959</td>\n",
       "      <td>2.353800</td>\n",
       "      <td>68.311100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c893f886a37f40968cb07a46a2698ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4275, training_loss=2.3625617775721857, metrics={'train_runtime': 5251.3908, 'train_samples_per_second': 6.513, 'train_steps_per_second': 0.814, 'total_flos': 4481357823344640.0, 'train_loss': 2.3625617775721857, 'epoch': 5.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c905d108-d060-4f87-a1ec-fc60742bd1dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFineTunedTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"FineTunedTransformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9d8c548-d1c1-4d52-b192-161dc543a2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c516db9ac3944fb4ba54745e4548c6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/7.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae9302ed22145dc96a9b28ac6116f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365a2671c2eb46e9aa47968cea9778f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/3.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ca8f258d1048a48668674b28179385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6275c3c3b8454c94c08e4dfae33f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426cc130fd96414a9d9d30cb3b675bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 7 LFS files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a359cbf1934e499426e6fd0969f387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa37278fca6048eb85f5c9a845b739e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dantepalacio/mbart-large-cc-lora-finetuned-kk2ru/commit/c58c169cc91bf77b509ab72e73627ddc8ddae915', commit_message='Upload folder using huggingface_hub', commit_description='', oid='c58c169cc91bf77b509ab72e73627ddc8ddae915', pr_url=None, repo_url=RepoUrl('https://huggingface.co/dantepalacio/mbart-large-cc-lora-finetuned-kk2ru', endpoint='https://huggingface.co', repo_type='model', repo_id='dantepalacio/mbart-large-cc-lora-finetuned-kk2ru'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "checkpoint_path = \"mbart-large-cc25-finetuned-kk-to-eu/checkpoint-4275\"\n",
    "repo_name = \"dantepalacio/mbart-large-cc-lora-finetuned-kk2ru\"\n",
    "\n",
    "# Загрузка модели на Hugging Face Hub\n",
    "api.upload_folder(\n",
    "    folder_path=checkpoint_path,\n",
    "    path_in_repo=\".\",\n",
    "    repo_id=repo_name,\n",
    "    repo_type=\"model\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
